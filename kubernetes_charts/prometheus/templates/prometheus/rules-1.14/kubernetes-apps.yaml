# Generated from 'kubernetes-apps' group from https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-rules.yaml
# Do not change in-place! In order to change this file first read following link:
# https://github.com/helm/charts/tree/master/stable/prometheus-operator/hack
{{- $kubeTargetVersion := default .Capabilities.KubeVersion.GitVersion .Values.kubeTargetVersionOverride }}
{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<1.16.0-0" $kubeTargetVersion) .Values.defaultRules.create .Values.kubeStateMetrics.enabled .Values.defaultRules.rules.kubernetesApps }}
apiVersion: {{ printf "%s/v1" (.Values.prometheusOperator.crdApiGroup | default "monitoring.coreos.com") }}
kind: PrometheusRule
metadata:
  name: {{ printf "%s-%s" (include "prometheus-operator.fullname" .) "kubernetes-apps" | trunc 63 | trimSuffix "-" }}
  labels:
    app: {{ template "prometheus-operator.name" . }}
{{ include "prometheus-operator.labels" . | indent 4 }}
{{- if .Values.defaultRules.labels }}
{{ toYaml .Values.defaultRules.labels | indent 4 }}
{{- end }}
{{- if .Values.defaultRules.annotations }}
  annotations:
{{ toYaml .Values.defaultRules.annotations | indent 4 }}
{{- end }}
spec:
  groups:
  - name: kubernetes-apps
    rules:
    - alert: KubePodCrashLooping
      annotations:
        message: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} ({{`{{ $labels.container }}`}}) is restarting {{`{{ printf "%.2f" $value }}`}} times for 2 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
      expr: rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[2m]) * 60 * 5 > 0
      for: 2m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubePodNotReady
      annotations:
        message: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been in a non-ready state for longer than a minute.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
      expr: sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Failed|Pending|Unknown", namespace=~"overprovisioning|loki-stack"}) > 0
      for: 15m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubePodNotReady
      annotations:
        message: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been in a non-ready state for longer than a minute.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
      expr: sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Failed|Pending|Unknown", namespace=~"jellibabix|monitoring|.*mysqlstreamer|elb-log-parser|infra"}) > 0
      for: 5m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubePodNotReady
      annotations:
        message: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been in a non-ready state for longer than 3 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
      expr: sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Failed|Pending|Unknown", namespace!~"jellibabix|loki-stack|monitoring|.*mysqlstreamer|elb-log-parser|infra|overprovisioning"}) > 0
      for: 3m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubeDeploymentGenerationMismatch
      annotations:
        message: Deployment generation for {{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment }}`}} does not match, this indicates that the Deployment has failed but has not been rolled back.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
      expr: |-
        kube_deployment_status_observed_generation{job="kube-state-metrics"}
          !=
        kube_deployment_metadata_generation{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubeDeploymentReplicasMismatchOverprovisioning
      annotations:
        message: Deployment {{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment }}`}} has not matched the expected number of replicas for longer than 5 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
      expr: |-
        (kube_deployment_spec_replicas{job="kube-state-metrics", namespace=~"overprovisioning"}) * 0.5
          >
        kube_deployment_status_replicas_available{job="kube-state-metrics", namespace=~"overprovisioning"}
      for: 5m
      labels:
        severity: critical
        namespace: 'overprovisioning'
    - alert: KubeDeploymentReplicasMismatch
      annotations:
        message: Deployment {{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment }}`}} has not matched the expected number of replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
      expr: |-
        kube_deployment_spec_replicas{job="kube-state-metrics"}
          !=
        kube_deployment_status_replicas_available{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubeStatefulSetReplicasMismatch
      annotations:
        message: StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} has not matched the expected number of replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch
      expr: |-
        kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
          !=
        kube_statefulset_status_replicas{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubeStatefulSetGenerationMismatch
      annotations:
        message: StatefulSet generation for {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} does not match, this indicates that the StatefulSet has failed but has not been rolled back.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch
      expr: |-
        kube_statefulset_status_observed_generation{job="kube-state-metrics"}
          !=
        kube_statefulset_metadata_generation{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubeStatefulSetUpdateNotRolledOut
      annotations:
        message: StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} update has not been rolled out.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout
      expr: |-
        max without (revision) (
          kube_statefulset_status_current_revision{job="kube-state-metrics"}
            unless
          kube_statefulset_status_update_revision{job="kube-state-metrics"}
        )
          *
        (
          kube_statefulset_replicas{job="kube-state-metrics"}
            !=
          kube_statefulset_status_replicas_updated{job="kube-state-metrics"}
        )
      for: 15m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubeDaemonSetRolloutStuck
      annotations:
        message: Only {{`{{ $value }}`}}% of the desired Pods of DaemonSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.daemonset }}`}} are scheduled and ready.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck
      expr: |-
        kube_daemonset_status_number_ready{job="kube-state-metrics"}
          /
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"} * 100 < 100
      for: 15m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubeDaemonSetNotScheduled
      annotations:
        message: '{{`{{ $value }}`}} Pods of DaemonSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.daemonset }}`}} are not scheduled.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled
      expr: |-
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
          -
        kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"} > 0
      for: 10m
      labels:
        severity: warning
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubeDaemonSetMisScheduled
      annotations:
        message: '{{`{{ $value }}`}} Pods of DaemonSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.daemonset }}`}} are running where they are not supposed to run.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled
      expr: kube_daemonset_status_number_misscheduled{job="kube-state-metrics"} > 0
      for: 15m
      labels:
        severity: warning
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubeCronJobRunning
      annotations:
        message: CronJob {{`{{ $labels.namespace }}`}}/{{`{{ $labels.cronjob }}`}} is taking more than 1h to complete.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecronjobrunning
      expr: time() - kube_cronjob_next_schedule_time{job="kube-state-metrics"} > 3600
      for: 1h
      labels:
        severity: warning
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubeJobCompletion
      annotations:
        message: Job {{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}} is taking more than one hour to complete.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion
      expr: kube_job_spec_completions{job="kube-state-metrics"} - kube_job_status_succeeded{job="kube-state-metrics"}  > 0
      for: 1h
      labels:
        severity: warning
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubeJobFailed
      annotations:
        message: Job {{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}} failed to complete.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed
      expr: kube_job_status_failed{job="kube-state-metrics"}  > 0
      for: 15m
      labels:
        severity: warning
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: JellibabixOBCallLag
      annotations:
        message: Pod {{`{{ $labels.namespace }}`}}/jellibabix_obcall_lag is lagging more that 55 seconds
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
      expr: sum by (namespace) (jellibabix_obcall_lag) > 55
      for: 5m
      labels:
        severity: critical
        namespace: 'jellibabix'
    - alert: JellibabixOBDLag
      annotations:
        message: Pod {{`{{ $labels.namespace }}`}}/jellibabix_obd_lag is lagging more that 40 seconds
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
      expr: sum by (namespace) (jellibabix_obd_lag) > 40
      for: 5m
      labels:
        severity: critical
        namespace: 'jellibabix'
    - alert: JellibabixOBDStatsLag
      annotations:
        message: Pod {{`{{ $labels.namespace }}`}}/jellibabix_obdstats_lag is lagging more that 40 seconds
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
      expr: sum by (namespace) (jellibabix_obdstats_lag) > 40
      for: 5m
      labels:
        severity: critical
        namespace: 'jellibabix'
    - alert: JellibabixTWCallLag
      annotations:
        message: Pod {{`{{ $labels.namespace }}`}}/jellibabix_twcall_lag is lagging more that 40 seconds
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
      expr: sum by (namespace) (jellibabix_twcall_lag) > 40
      for: 5m
      labels:
        severity: critical
        namespace: 'jellibabix'
    - alert: JellibabixTWSMSLag
      annotations:
        message: Pod {{`{{ $labels.namespace }}`}}/jellibabix_twsms_lag is lagging more that 40 seconds
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
      expr: sum by (namespace) (jellibabix_twsms_lag) > 40
      for: 5m
      labels:
        severity: critical
        namespace: 'jellibabix'
    - alert: KubeNodeNotReady
      annotations:
        message: Kubernetes node is not responding (instance {{`{{$labels.instance}}`}})
        runbook_url: ""
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 15m
      labels:
        severity: critical
        namespace: 'infra'
    - alert: KubeNodeMemoryPressure
      annotations:
        message: Kubernetes node has a high memory pressure (instance {{`{{ $labels.instance }}`}})
        runbook_url: ""
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 3m
      labels:
        severity: critical
        namespace: infra
    - alert: High_4xx_API_Count
      annotations:
        message: The service {{`{{ $labels.destination_service_namespace }}`}} is responding with Status code {{`{{ $labels.response_code }}`}} for 20% of its total traffic over last 5 minutes
        runbook_url: ""
      expr: sum by (destination_service_namespace,response_code) (rate(istio_requests_total{response_code=~"4.*",destination_service_namespace != "unknown"}[5m])) > on (destination_service_namespace) group_left .2 * sum by (destination_service_namespace) (rate(istio_requests_total[5m]))
      for: 2m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.destination_service_namespace }}`}}'
    - alert: High_5xx_API_Count
      annotations:
        message: The service {{`{{ $labels.destination_service_namespace }}`}} is responding with Status code {{`{{ $labels.response_code }}`}} for 10% of its total traffic over last 5 minutes
        runbook_url: ""
      expr: sum by (destination_service_namespace,response_code) (rate(istio_requests_total{response_code=~"5.*",destination_service_namespace != "unknown"}[5m])) > on (destination_service_namespace) group_left .1 * sum by (destination_service_namespace) (rate(istio_requests_total[5m]))
      for: 2m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.destination_service_namespace }}`}}'

    - alert: PushgatewayMetricsMissing
      annotations:
        message: Pushgateway metrics for {{`{{ $labels.job }}`}} is missing 
        runbook_url: "Please check the Prometheus pushgateway integration with Prometheus"
      expr: absent({service="prometheus-pushgateway", job=~"jellibabix.*"}) == 1
      for: 5m
      labels:
        severity: critical
        namespace: infra
    - alert: PodMemoryUtilization
      annotations:
        message: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} ({{`{{ $labels.container }}`}}) reached {{`{{ printf "%.2f" $value }}`}} of memory limit 
        runbook_url: ""
      expr: (100*avg(container_memory_working_set_bytes{container!="POD", namespace!= "infra"}) by (pod,namespace,container))/ on (pod,namespace,container) (avg(kube_pod_container_resource_limits_memory_bytes) by (pod,namespace,container)) > 80 or (100*avg(container_memory_working_set_bytes{container!="POD", namespace= "infra"}) by (pod,namespace,container))/ on (pod,namespace,container) (avg(kube_pod_container_resource_limits_memory_bytes) by (pod,namespace,container)) > 90
      for: 5m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: PodCPUUtilization
      annotations:
        message: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} ({{`{{ $labels.container }}`}}) reached {{`{{ printf "%.2f" $value }}`}} of cpu limit 
        runbook_url: ""
      expr: (100*avg(rate(container_cpu_usage_seconds_total{image!="", container!="POD"}[5m])) by (pod,container,namespace)) / on (pod,container,namespace) avg(kube_pod_container_resource_limits_cpu_cores) by (pod,container,namespace) > 80
      for: 5m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.namespace }}`}}'
    - alert: KubePodOOMKilled
      annotations:
        message: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} got oom killed.
        runbook_url: {{ .Values.defaultRules.runbookUrl }}alert-name-oomkilled
      expr: kube_pod_container_status_terminated_reason{reason="OOMKilled",job=~"kubernetes-service-endpoints|kube-state-metrics", namespace=~"reportix.*"} == 1
      for: 5m
      labels:
        severity: critical
        namespace: '{{`{{ $labels.namespace }}`}}'
    
{{- end }}
